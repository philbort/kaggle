{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "[['1' '0' '3' ..., '7.25' '' 'S']\n",
      " ['2' '1' '1' ..., '71.2833' 'C85' 'C']\n",
      " ['3' '1' '3' ..., '7.925' '' 'S']\n",
      " ..., \n",
      " ['889' '0' '3' ..., '23.45' '' 'S']\n",
      " ['890' '1' '1' ..., '30' 'C148' 'C']\n",
      " ['891' '0' '3' ..., '7.75' '' 'Q']]\n"
     ]
    }
   ],
   "source": [
    "# Open up the csv file in to a Python object\n",
    "csv_file_object = csv.reader(open('train.csv', 'rb')) \n",
    "# The next() command just skips the first line which is a header\n",
    "header = csv_file_object.next()  \n",
    "\n",
    "data=[]\n",
    "for row in csv_file_object:\n",
    "    data.append(row)\n",
    "# Convert from a list to an array\n",
    "data = np.array(data)   \n",
    "\n",
    "# Be aware that each item is currently a string in this format\n",
    "print(header)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Class_1</th>\n",
       "      <th>Class_2</th>\n",
       "      <th>Class_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>-0.132980</td>\n",
       "      <td>-0.229728</td>\n",
       "      <td>0.647587</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.206510</td>\n",
       "      <td>0.551066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.162746</td>\n",
       "      <td>0.272966</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>0.428790</td>\n",
       "      <td>0.405028</td>\n",
       "      <td>0.497665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.494750</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.225000</td>\n",
       "      <td>-0.420896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>-0.355458</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>-0.190000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived         Age        Fare      Gender  \\\n",
       "count   891.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean    446.000000    0.383838   -0.132980   -0.229728    0.647587   \n",
       "std     257.353842    0.486592    0.162746    0.272966    0.477990   \n",
       "min       1.000000    0.000000   -0.494750   -0.500000    0.000000   \n",
       "25%     223.500000    0.000000   -0.225000   -0.420896    0.000000   \n",
       "50%     446.000000    0.000000   -0.150000   -0.355458    1.000000   \n",
       "75%     668.500000    1.000000   -0.062500   -0.190000    1.000000   \n",
       "max     891.000000    1.000000    0.500000    0.500000    1.000000   \n",
       "\n",
       "          Class_1     Class_2     Class_3  \n",
       "count  891.000000  891.000000  891.000000  \n",
       "mean     0.242424    0.206510    0.551066  \n",
       "std      0.428790    0.405028    0.497665  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000  \n",
       "50%      0.000000    0.000000    1.000000  \n",
       "75%      0.000000    0.000000    1.000000  \n",
       "max      1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_clean_data(csv_file):\n",
    "    df = pd.read_csv(csv_file, header = 0)\n",
    "    # Normalize \"Age\"\n",
    "    df['Age'] = df['Age'].fillna(df.Age.median())\n",
    "    df['Age'] = (df['Age'] - 40)/80\n",
    "    # One hot encoding for \"Sex\"\n",
    "    df['Gender'] = df['Sex'].map({'female':0, 'male':1}).astype(int)\n",
    "    # Normalize \"Fare\"\n",
    "    df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
    "    df.loc[df['Fare'] > 100,'Fare'] = 100\n",
    "    df['Fare'] = (df['Fare'] - 50)/100\n",
    "    df['Embarked'] = df['Embarked'].fillna('S')\n",
    "    df = df.drop(['SibSp','Parch','Name','Sex','Ticket','Cabin','Embarked'],axis=1)\n",
    "    # One hot encoding for \"Pclass\"\n",
    "    df = df.join(pd.get_dummies(df['Pclass'], prefix ='Class').astype(int))\n",
    "    df = df.drop(['Pclass'], axis = 1)\n",
    "    return df\n",
    "train_df = load_clean_data('train.csv')\n",
    "test_df = load_clean_data('test.csv')\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_data = train_df.drop(['PassengerId', \"Survived\"], axis = 1)\n",
    "y_data = train_df['Survived']\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "test_data = test_df.drop(['PassengerId'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "feature_size = 6\n",
    "num_labels = 2\n",
    "hidden_nodes_size = 5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    '''\n",
    "    Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch.  \n",
    "    '''\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, feature_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(x_valid.as_matrix(), dtype=tf.float32)\n",
    "    tf_valid_labels = tf.constant(y_valid.as_matrix(),dtype=tf.float32)\n",
    "    tf_test_dataset = tf.constant(test_data.as_matrix(),dtype=tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([6, hidden_nodes_size]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes_size]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_nodes_size, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1), weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden, weights2) + biases2)\n",
    "    \n",
    "    test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-da8ee990dffa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Generate a minibatch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         '''\n\u001b[1;32m     16\u001b[0m         \u001b[0mPrepare\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mtelling\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfeed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "num_steps = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps+1):\n",
    "        '''\n",
    "        Pick an offset within the training data, which has been randomized.\n",
    "        Note: we could use better randomization across epochs.\n",
    "        '''\n",
    "        offset = (step * batch_size) % (y_train.as_matrix().shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = x_train.as_matrix()[offset:(offset + batch_size), :]\n",
    "        batch_labels = y_train.as_matrix()[offset:(offset + batch_size), :]\n",
    "        '''\n",
    "        Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\n",
    "        '''\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
     ]
    }
   ],
   "source": [
    "test_data = test_df.drop(['PassengerId'], axis = 1)\n",
    "y_pred = classifier.predict(test_data)\n",
    "submission = pd.DataFrame({ \"PassengerId\":test_df['PassengerId'],\n",
    "                             \"Survived\":y_pred })\n",
    "submission.to_csv('titanic_tf_learn.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
